# -*- coding: utf-8 -*-
"""Experiment 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6EVcUpq-HQdVJyAPgqJXf-xxk5ijXfa
"""

# Install NLTK library
!pip install nltk

# Import NLTK and download necessary resources
import nltk
nltk.download('punkt')

# Import required tokenizers
from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize, WordPunctTokenizer, TabTokenizer

# Sample text for tokenization
sample_text = "Hello , My name is Aashish , Today is very beautiful day."

# Word Tokenization
word_tokens = word_tokenize(sample_text)
print("Word Tokens (WordTokenizer):", word_tokens)

# Sentence Tokenization
sent_tokens = sent_tokenize(sample_text)
print("\nSentence Tokens (SentenceTokenizer):", sent_tokens)

# Regular Expression Tokenization
# Define regular expression pattern for tokenization
pattern = r'\w+'
regex_tokens = regexp_tokenize(sample_text, pattern)
print("\nRegular Expression Tokens (RegExpTokenizer):", regex_tokens)

# WordPunct Tokenization
wordpunct_tokenizer = WordPunctTokenizer()
wordpunct_tokens = wordpunct_tokenizer.tokenize(sample_text)
print("\nWordPunct Tokens (WordPunctTokenizer):", wordpunct_tokens)

# Tab Tokenization
tab_tokenizer = TabTokenizer()
tab_tokens = tab_tokenizer.tokenize(sample_text)
print("\nTab Tokens (TabTokenizer):", tab_tokens)